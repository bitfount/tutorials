{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c36db8",
   "metadata": {},
   "source": [
    "# Inference-Only Models: Custom Models for Federated Inference\n",
    "\n",
    "Welcome to building custom models for federated inference! With Bitfount's inference model classes, you can create powerful federated inference models with minimal code and complexity.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Bitfount provides two base classes specifically designed for inference-only tasks:\n",
    "\n",
    "- **`PytorchLightningInferenceModel`** - PyTorch Lightning-based inference model\n",
    "- **`PytorchInferenceModel`** - Simple PyTorch inference model without Lightning dependencies\n",
    "\n",
    "These classes eliminate the need to implement training-related methods, making it easy to deploy pre-trained models for federated inference.\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitfount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232db43",
   "metadata": {},
   "source": [
    "### Getting Started with Inference Models\n",
    "\n",
    "With Bitfount's Pytorch inference base classes, you only need to implement **one method**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitfount.backends.pytorch.models.inference_model import PytorchInferenceModel\n",
    "\n",
    "\n",
    "class MySimpleInferenceModel(PytorchInferenceModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"The only method you need to implement!\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a0783",
   "metadata": {},
   "source": [
    "## Image Classification with PyTorch\n",
    "\n",
    "Let's set up the environment and create a complete inference model using a pre-trained ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c14de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import nest_asyncio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from bitfount import (\n",
    "    BitfountModelReference,\n",
    "    BitfountSchema,\n",
    "    ImageSource,\n",
    "    DataStructure,\n",
    "    ModelInference,\n",
    "    ResultsOnly,\n",
    "    get_pod_schema,\n",
    "    setup_loggers,\n",
    ")\n",
    "from bitfount.backends.pytorch.models.inference_model import (\n",
    "    PytorchLightningInferenceModel,\n",
    "    PytorchInferenceModel,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()  # Needed because Jupyter also has an asyncio loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f430d1",
   "metadata": {
    "tags": [
     "logger_setup"
    ]
   },
   "outputs": [],
   "source": [
    "loggers = setup_loggers([logging.getLogger(\"bitfount\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27f29e",
   "metadata": {},
   "source": [
    "Now let's create our inference model by first saving it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from bitfount.backends.pytorch.models.inference_model import PytorchInferenceModel\n",
    "\n",
    "\n",
    "class ResNetInferenceModel(PytorchInferenceModel):\n",
    "    \"\"\"Simple inference model for image classification.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int = 10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def create_model(self) -> nn.Module:\n",
    "        \"\"\"Create and return a simple CNN model.\"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.n_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a58be2",
   "metadata": {},
   "source": [
    "### Testing Locally with Image Data\n",
    "\n",
    "Let's test our model locally first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35516802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some image data for testing\n",
    "datasource = ImageSource(\n",
    "    path=\"sample_images/\",  # Path to your image directory\n",
    ")\n",
    "\n",
    "schema = BitfountSchema(\n",
    "    name=\"image-inference-demo\",\n",
    ")\n",
    "\n",
    "# For image data, specify image columns\n",
    "force_stypes = {\n",
    "    \"image\": [\"Pixel Data\"],  # Standard image column name\n",
    "}\n",
    "schema.generate_full_schema(datasource, force_stypes=force_stypes)\n",
    "\n",
    "# Create datastructure for inference (no target needed)\n",
    "datastructure = DataStructure(\n",
    "    target=None,  # No target for inference, can be skipped\n",
    "    image_cols=[\"Pixel Data\"],  # Specify the image column\n",
    "    selected_cols=[\"Pixel Data\"],  # Add selected columns\n",
    "    # schema_requirements=\"full\"  # Optional\n",
    ")\n",
    "\n",
    "# Initialize our model\n",
    "model = ResNetInferenceModel(\n",
    "    datastructure=datastructure,\n",
    "    schema=schema,\n",
    "    n_classes=2,\n",
    "    batch_size=2,  # Adjust batch size as needed\n",
    ")\n",
    "\n",
    "# Test local inference\n",
    "model.initialise_model(datasource)\n",
    "local_results = model.predict(data=datasource)\n",
    "\n",
    "print(f\"Local inference completed! Got {len(local_results.preds)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b0c3c",
   "metadata": {},
   "source": [
    "### Run Inference on a Pod\n",
    "\n",
    "Now use your simple model with the existing Bitfount infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the image dataset pod\n",
    "pod_identifier = \"image-datasource\"\n",
    "schema = get_pod_schema(pod_identifier)\n",
    "\n",
    "# Create model reference\n",
    "model_ref = BitfountModelReference(\n",
    "    model_ref=Path(\"ResNetInferenceModel.py\"),  # Your simple model file\n",
    "    datastructure=datastructure,\n",
    "    schema=schema,\n",
    ")\n",
    "# Run federated inference\n",
    "protocol = ResultsOnly(algorithm=ModelInference(model=model_ref))\n",
    "results = protocol.run(pod_identifiers=[pod_identifier])\n",
    "\n",
    "print(\"Inference completed!\")\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce572936",
   "metadata": {},
   "source": [
    "Similar approach can be used for creating a model inheriting from `PytorchLightningInferenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from bitfount.backends.pytorch.models.inference_model import (\n",
    "    PytorchLightningInferenceModel,\n",
    ")\n",
    "\n",
    "\n",
    "class ResNetLightningInferenceModel(PytorchLightningInferenceModel):\n",
    "    \"\"\"Simple inference model for image classification.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int = 10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def create_model(self) -> nn.Module:\n",
    "        \"\"\"Create and return a simple CNN model.\"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.n_classes),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573954cb",
   "metadata": {},
   "source": [
    "Let's test the Lightning model locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some image data for testing\n",
    "datasource = ImageSource(\n",
    "    path=\"sample_images/\",  # Path to your image directory\n",
    ")\n",
    "\n",
    "schema = BitfountSchema(\n",
    "    name=\"image-inference-demo\",\n",
    ")\n",
    "\n",
    "# For image data, specify image columns\n",
    "force_stypes = {\n",
    "    \"image\": [\"Pixel Data\"],  # Standard image column name\n",
    "}\n",
    "schema.generate_full_schema(datasource, force_stypes=force_stypes)\n",
    "\n",
    "# Create datastructure for inference (no target needed)\n",
    "datastructure = DataStructure(\n",
    "    target=None,  # No target for inference, can be skipped\n",
    "    image_cols=[\"Pixel Data\"],  # Specify the image column\n",
    "    selected_cols=[\"Pixel Data\"],  # Add selected columns\n",
    ")\n",
    "\n",
    "# Initialize our model\n",
    "model = ResNetLightningInferenceModel(\n",
    "    datastructure=datastructure,\n",
    "    schema=schema,\n",
    "    n_classes=2,\n",
    "    batch_size=2,  # Adjust batch size as needed\n",
    ")\n",
    "\n",
    "# Test local inference\n",
    "model.initialise_model(datasource)\n",
    "local_lightning_results = model.predict(data=datasource)\n",
    "\n",
    "print(\n",
    "    f\"Local inference completed! Got {len(local_lightning_results.preds)} predictions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae0092",
   "metadata": {},
   "source": [
    "## Understanding the Two Base Classes\n",
    "\n",
    "### PytorchLightningInferenceModel vs PytorchInferenceModel\n",
    "\n",
    "Both classes provide the same core functionality but with different underlying architectures:\n",
    "\n",
    "| Feature                 | PytorchLightningInferenceModel                | PytorchInferenceModel    |\n",
    "| ----------------------- | --------------------------------------------- | ------------------------ |\n",
    "| **Dependencies**        | Requires PyTorch Lightning                    | Pure PyTorch only        |\n",
    "| **Execution**           | Uses Lightning Trainer                        | Direct PyTorch execution |\n",
    "| **GPU/Device Handling** | Lightning's automatic device management       | Custom device detection  |\n",
    "| **Extensibility**       | Full Lightning ecosystem (callbacks, loggers) | Simple, direct control   |\n",
    "\n",
    "### Key Architectural Differences\n",
    "\n",
    "The main difference lies in how the `predict()` method is implemented.\n",
    "\n",
    "**PytorchLightningInferenceModel:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9657552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses PyTorch Lightning under the hood\n",
    "def predict(self, data=None, **kwargs):\n",
    "    # Uses pl.Trainer.test() internally\n",
    "    self._pl_trainer.test(model=self, dataloaders=self.test_dl)\n",
    "    return PredictReturnType(preds=self._test_preds, keys=self._test_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82178ccd",
   "metadata": {},
   "source": [
    "**PytorchInferenceModel:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct PyTorch execution\n",
    "def predict(self, data=None, **kwargs):\n",
    "    # Direct batch processing with torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        for batch in self.test_dl:\n",
    "            predictions = self.forward(batch_data)\n",
    "            # Process predictions...\n",
    "    return PredictReturnType(preds=all_predictions, keys=all_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0162b94",
   "metadata": {},
   "source": [
    "## When to Use Which Base Class\n",
    "\n",
    "**Use `PytorchLightningInferenceModel` when:**\n",
    "\n",
    "- You want full PyTorch Lightning integration\n",
    "- You need Lightning's advanced features (callbacks, logging, etc.)\n",
    "- You want to leverage our dataloaders and datasets\n",
    "- You prefer Lightning's structured approach to model organization\n",
    "\n",
    "**Use `PytorchInferenceModel` when:**\n",
    "\n",
    "- You want minimal dependencies and faster startup\n",
    "- You prefer simple, direct PyTorch code\n",
    "- You're building lightweight inference services\n",
    "- You need fine-grained control over the inference loop\n",
    "- You're deploying in resource-constrained environments\n",
    "\n",
    "## Advanced Customization: Overriding Methods\n",
    "\n",
    "While you only **need** to implement `create_model()`, you can override other methods for custom behavior:\n",
    "\n",
    "### Customizing Your Inference Models\n",
    "\n",
    "Both base classes share common functionality but have different advanced hooks available.\n",
    "\n",
    "## Common Base Functionality (Both Classes)\n",
    "\n",
    "`create_model()`**- Required Method**\n",
    "\n",
    "**Every inference model must implement this abstract method:**\n",
    "\n",
    "- Return your PyTorch model architecture (`nn.Module`)\n",
    "- Called automatically during model initialization\n",
    "- The model will be moved to appropriate device and set to evaluation mode\n",
    "\n",
    "### Shared Public Methods Available for Override:\n",
    "\n",
    "`initialise_model(data, data_splitter, context)` **- Model Setup**\n",
    "\n",
    "**Default behavior:**\n",
    "\n",
    "- Prepares the model for inference\n",
    "- Creates data loaders from provided datasource\n",
    "- Calls `create_model()` to instantiate your model\n",
    "- Sets up the inference pipeline\n",
    "\n",
    "**When to override:**\n",
    "\n",
    "- Custom model initialization logic\n",
    "\n",
    "`forward(x)` **- Model Forward Pass**\n",
    "\n",
    "**Default behavior:**\n",
    "\n",
    "- Handles single and multi-image column scenarios\n",
    "- Runs input through your created model\n",
    "- Returns model predictions\n",
    "\n",
    "**When to override:**\n",
    "\n",
    "- Custom input preprocessing\n",
    "- Multi-model ensemble logic\n",
    "- Special output formatting needs\n",
    "\n",
    "### Shared Utility Methods:\n",
    "\n",
    "`split_dataloader_output(data)` **- Data Parsing**\n",
    "\n",
    "**Purpose:** Properly extracts input data from dataloader output\n",
    "**When to use:** Processing batch data in custom methods instead of manual parsing\n",
    "\n",
    "`serialize(filename)` **and** `deserialize(content)` **- Model Persistence**\n",
    "\n",
    "**Purpose:** Save and load trained model weights\n",
    "**Usage:** Standard model checkpointing and deployment\n",
    "\n",
    "## PytorchLightningInferenceModel Customization\n",
    "\n",
    "### Lightning-Specific Override Methods:\n",
    "\n",
    "`test_step(batch, batch_idx)` **- Per-Batch Processing**\n",
    "\n",
    "**Default behavior:**\n",
    "\n",
    "- Processes each batch during inference\n",
    "- Extracts data and optional keys from batch\n",
    "- Runs forward pass and collects results\n",
    "- Handles prediction aggregation automatically\n",
    "\n",
    "**When to override:**\n",
    "\n",
    "- Custom preprocessing per batch\n",
    "- Ensemble predictions across multiple models\n",
    "- Custom metrics or logging during inference\n",
    "- Special batch result formatting\n",
    "\n",
    "`on_test_epoch_end()` **- End-of-Inference Processing**\n",
    "\n",
    "**Default behavior:**\n",
    "\n",
    "- Aggregates all batch results\n",
    "- Prepares final prediction outputs\n",
    "- Handles key-prediction alignment\n",
    "\n",
    "**When to override:**\n",
    "\n",
    "- Custom result aggregation logic\n",
    "- Post-inference processing steps\n",
    "- Custom validation or filtering\n",
    "\n",
    "`predict(data, **kwargs)` **- Complete Pipeline Control**\n",
    "\n",
    "**Default behavior:**\n",
    "\n",
    "- Uses Lightning trainer for inference execution\n",
    "- Manages the complete inference workflow\n",
    "- Returns formatted prediction results\n",
    "\n",
    "### Lightning Benefits:\n",
    "\n",
    "- Automatic device management through trainer\n",
    "- Built-in logging and metrics capabilities\n",
    "- Structured approach with hooks and callbacks\n",
    "- Easy integration with Lightning ecosystem\n",
    "\n",
    "## PytorchInferenceModel Customization\n",
    "\n",
    "### Inference Model Override Methods:\n",
    "\n",
    "`predict(data, **kwargs)` **- Direct Inference Control**\n",
    "\n",
    "**Default behavior:**\n",
    "\n",
    "- Manual batch processing loop with `torch.no_grad()`\n",
    "- Direct device management and model evaluation\n",
    "- Explicit prediction collection and formatting\n",
    "- No Lightning trainer dependency\n",
    "\n",
    "**When to override:**\n",
    "\n",
    "- Fine-grained control over inference loop\n",
    "- Custom batch processing logic\n",
    "- Memory-efficient streaming inference\n",
    "- Integration with non-Lightning workflows\n",
    "\n",
    "### Simple Model Benefits:\n",
    "\n",
    "- No PyTorch Lightning dependency\n",
    "- Direct PyTorch control and transparency\n",
    "- Explicit device and memory management\n",
    "- Faster startup and execution\n",
    "\n",
    "## Method Override Guidelines\n",
    "\n",
    "### Start Simple:\n",
    "\n",
    "1. Implement only `create_model()`\n",
    "2. Test basic inference functionality\n",
    "3. Add method overrides only when needed\n",
    "\n",
    "### Lightning Model Progression:\n",
    "\n",
    "1. Override `test_step()` for batch-level customization\n",
    "2. Override `on_test_epoch_end()` for result aggregation\n",
    "3. Override `predict()` for complete pipeline control\n",
    "\n",
    "### Simple Model Progression:\n",
    "\n",
    "1. Override `forward()` for input/output processing\n",
    "2. Override `initialise_model()` for setup customization\n",
    "3. Override `predict()` for complete pipeline control\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Choose the Right Base**: Lightning for research, Simple for production\n",
    "2. **Always call `model.eval()`** in your `create_model()` method\n",
    "3. **Start Minimal**: Begin with just `create_model()`, add complexity incrementally\n",
    "4. **Use Utilities**: Leverage `split_dataloader_output()` for robust data handling\n",
    "5. **Test Locally**: Validate all customizations before federated deployment\n",
    "6. **Handle Edge Cases**: Consider different input formats and error conditions\n",
    "7. **Document Changes**: Comment custom logic for team collaboration\n",
    "\n",
    "You've now learned how to create simple, powerful inference models for federated learning with Bitfount!\n",
    "\n",
    "Contact our support team at [support@bitfount.com](mailto:support@bitfount.com) if you have any questions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "hide_notebook_metadata": true,
   "root_level_metadata": {
    "hide_title": true,
    "sidebar_label": "Inference-Only Models",
    "sidebar_position": 2,
    "slug": "/advanced-data-science-tasks/inference-only-models"
   },
   "root_level_metadata_as_raw_cell": false
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
