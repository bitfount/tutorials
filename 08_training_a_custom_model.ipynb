{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb312ad0",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bitfount/tutorials/main?labpath=08_training_a_custom_model.ipynb)\n",
    "\n",
    "# Federated Learning - Part 8: Using custom models\n",
    "\n",
    "Welcome to the Bitfount federated learning tutorials! In this sequence of tutorials, you will learn how federated learning works on the Bitfount platform. This is the eighth notebook in the series.\n",
    "\n",
    "In this tutorial you will learn how to train a model using a custom model by extending a base model in the Bitfount framework. We will use the pod you set up in Part 1, so make sure you run have it first.\n",
    "\n",
    "### 1.1 Setting everything up\n",
    "\n",
    "Let's import the relevant pieces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging  # isort: split\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from bitfount import (\n",
    "    BitfountSchema,\n",
    "    CSVSource,\n",
    "    DataStructure,\n",
    "    PyTorchBitfountModel,\n",
    "    PyTorchClassifierMixIn,\n",
    "    get_pod_schema,\n",
    ")\n",
    "from bitfount.runners.utils import setup_loggers\n",
    "\n",
    "nest_asyncio.apply()  # Needed because Jupyter also has an asyncio loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736230e",
   "metadata": {},
   "source": [
    "Let's set up the loggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c7c56",
   "metadata": {
    "tags": [
     "logger_setup"
    ]
   },
   "outputs": [],
   "source": [
    "loggers = setup_loggers([logging.getLogger(\"bitfount\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b801897",
   "metadata": {},
   "source": [
    "### 1.2 Creating a custom model\n",
    "\n",
    "For this tutorial we will be creating a custom model, extending and overriding the built-in `BitfountModel` class (in particular we will be using the `PyTorchBitfountModel` class). Details on this can be found in the documentation in the `bitfount.backends.pytorch.models.bitfount_model` module.\n",
    "\n",
    "The `PyTorchBitfountModel` uses the [PyTorch Lightning](https://www.pytorchlightning.ai/) library to provide high-level implementation options for a model in the PyTorch framework. This enables you to only have to implement the methods you need to dictate how the model training should be performed.\n",
    "\n",
    "For our custom model we _need_ to implement the following methods:\n",
    "\n",
    "- `__init__()`: how to setup the model\n",
    "- `configure_optimizers()`: how optimizers should be configured in the model\n",
    "- `forward()`: how to perform a forward pass in the model, how the loss is calculated\n",
    "- `training_step()`: what one training step in the model looks like\n",
    "- `validation_step()`: what one validation step in the model looks like\n",
    "- `test_step()`: what one test step in the model looks like\n",
    "\n",
    "Now we implement the custom model, feel free to try out your own model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d58805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the class name for your Custom model\n",
    "class MyCustomModel(PyTorchClassifierMixIn, PyTorchBitfountModel):\n",
    "    # A custom model built using PyTorch Lightning.\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.learning_rate = 0.001\n",
    "        # Initializes the model and sets hyperparameters.\n",
    "        # We need to call the parent __init__ first to ensure base model is set up.\n",
    "        # Then we can set our custom model parameters.\n",
    "\n",
    "    def create_model(self):\n",
    "        self.input_size = self.datastructure.input_size\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(500, self.n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines the operations we want to use for prediction.\n",
    "        x, sup = x\n",
    "        x = self._model(x.float())\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Computes and returns the training loss for a batch of data.\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Operates on a single batch of data from the validation set.\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "        preds = F.softmax(preds, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        # We can log out some useful stats so we can see progress\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return {\n",
    "            \"val_loss\": loss,\n",
    "            \"val_acc\": acc,\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Operates on a single batch of data from the test set.\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "        # We add these actual values and predictions to the\n",
    "        # `self.targs` and `self.preds` lists.\n",
    "        self.targs.extend(y.tolist())\n",
    "        self.preds.extend(F.softmax(preds, dim=1).tolist())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Configure the optimizer we wish to use whilst training.\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08d0e8",
   "metadata": {},
   "source": [
    "### 1.3 Training locally with a custom model\n",
    "\n",
    "With the above model we can now change our config to use this custom model. The configuration is for the most part the same as before.\n",
    "\n",
    "First, let's import and test the model on a local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = CSVSource(\n",
    "    path=\"https://bitfount-hosted-downloads.s3.eu-west-2.amazonaws.com/bitfount-tutorials/census_income.csv\",\n",
    "    ignore_cols=[\"fnlwgt\"],\n",
    ")\n",
    "schema = BitfountSchema(\n",
    "    datasource,\n",
    "    table_name=\"census-income-demo\",\n",
    "    force_stypes={\n",
    "        \"census-income-demo\": {\n",
    "            \"categorical\": [\n",
    "                \"TARGET\",\n",
    "                \"workclass\",\n",
    "                \"marital-status\",\n",
    "                \"occupation\",\n",
    "                \"relationship\",\n",
    "                \"race\",\n",
    "                \"native-country\",\n",
    "                \"gender\",\n",
    "                \"education\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "datastructure = DataStructure(target=\"TARGET\", table=\"census-income-demo\")\n",
    "model = MyCustomModel(datastructure=datastructure, schema=schema, epochs=2)\n",
    "model.fit(data=datasource)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e37302",
   "metadata": {},
   "source": [
    "### 1.4 Training on a pod with a custom model\n",
    "\n",
    "With the model file created we can now change the yaml config to use this custom model. The configuration is for the most part the same as before but you will note that we now specify `bitfount_model` rather than `name` in the `model` section.\n",
    "\n",
    "Within this `bitfount_model` section you can specify `username` and `model_ref`. In our case, the username is our own username so we don't need to specify it but if you wanted to use a model uploaded by someone else you can specify their username and the name of their model.\n",
    "\n",
    "`model_ref` is either the name of an existing custom model (one that has been uploaded to the hub) or, if using a new custom model, the path to the model file. The Modeller code will handle the upload of the model to the hub the first time it is used, after which you could just refer to it by name instead.\n",
    "\n",
    "The pods that we are training on will identify that this is a custom model and retrieve the model file from the hub to use this new model. This allows you to extend and improve on the base models that are included in every pod.\n",
    "\n",
    "That's all the setup, let's run the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_identifier = \"census-income-demo\"\n",
    "schema = get_pod_schema(pod_identifier)\n",
    "model.fit(\n",
    "    pod_identifiers=[pod_identifier],\n",
    "    model_out=Path(\"part_8_model.pt\"),\n",
    "    extra_imports=[\"from torchmetrics.functional import accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5b755",
   "metadata": {},
   "source": [
    "If you are following the tutorials in Binder, make sure the sidebar is displayed by clicking the folder icon on the left of the screen. Here you will be able to navigate to the next tutorial."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb",
   "hide_notebook_metadata": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
